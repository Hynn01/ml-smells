<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DSLinter - Linter for Machine Learning - Specific Code Smells</title>
    <link>https://hynn01.github.io/dslinter/</link>
    <description>Recent content on DSLinter - Linter for Machine Learning - Specific Code Smells</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://hynn01.github.io/dslinter/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Randomness Uncontrolled</title>
      <link>https://hynn01.github.io/dslinter/code-smells/randomness-uncontrolled/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/randomness-uncontrolled/</guid>
      <description>Description Debugging is easier if the results are reproducible when developing ML systems. Also, reproducibility helps conduct studies based on previous models. Setting random seeds significantly contributes to the reproducibility of ML applications. There are several scenes that a random seed is involved. In Scikit-Learn, randomness is inherently involved in some estimators(e.g., Random Forest) and cross-validation splitters. If the random seed is not set, the random forest algorithm might provide a different result every time it runs, and the dataset split by cross-validation splitter will also be different next time it runs.</description>
    </item>
    
    <item>
      <title>In Place APIs Misused</title>
      <link>https://hynn01.github.io/dslinter/code-smells/in-place-apis-misused/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/in-place-apis-misused/</guid>
      <description>Description “In-place operation is an operation that directly changes the content of a given linear algebra, vector, matrices (Tensor) without making a copy.” Due to the nature of the in-place operation, the in-place APIs are easily misused. Developers sometimes forget to set the in-place parameter in APIs to true while not assigning the new result to a variable, causing potential silent bugs. The data is not updated in this way, but the developer thinks it is and might not be able to find where the bug is.</description>
    </item>
    
    <item>
      <title>Unnecessary Iteration</title>
      <link>https://hynn01.github.io/dslinter/code-smells/unnecessary-iteration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/unnecessary-iteration/</guid>
      <description>Description &amp;ldquo;Vectorization is the process of converting an algorithm from operating on a single value at a time to operating on a set of values (vector) at one time.&amp;rdquo; ML applications are often data-intensive and need to apply an operation on a dataset. Therefore, it is better to adopt vectorized solution instead of iterating over data. As stated in the Pandas documentation : ”Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided”.</description>
    </item>
    
    <item>
      <title>No Scaling Before Scaling-sensitive Operation</title>
      <link>https://hynn01.github.io/dslinter/code-smells/no-scaling-before-scaling-sensitive-operation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/no-scaling-before-scaling-sensitive-operation/</guid>
      <description>Description Principle Component Analysis (PCA) is used for finding the components that maximize the data&amp;rsquo;s variation and reduce its dimensions, which is an essential data processing method. Scaling is pretty crucial to PCA because of the way the principal components are calculated. If one variable is on a larger scale than another, it will dominate the PCA procedure. Similarly, there are some other scaling-sensitive operations. Support Vector Machine (SVM), Stochastic Gradient Descent (SGD), Multi-layer Perceptron classifier, L1 and L2 regularization are all sensitive to feature scaling.</description>
    </item>
    
    <item>
      <title>Data Leakage</title>
      <link>https://hynn01.github.io/dslinter/code-smells/data-leakage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/data-leakage/</guid>
      <description>Description &amp;ldquo;Data Leakage happens when the data you are using to train a machine learning algorithm happens to have the information you are trying to predict.&amp;rdquo; It results in overly optimistic performance during testing and poor performance in real-world usage. There are two main sources of data leakage: leaky predictors and a leaky validation strategy. Leaky predictors refer to the situation where some features updated or created after the target value is realized are included.</description>
    </item>
    
    <item>
      <title>Hyperparameter not Explicitly Set</title>
      <link>https://hynn01.github.io/dslinter/code-smells/hyperparameter-not-explicitly-set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/hyperparameter-not-explicitly-set/</guid>
      <description>Description Hyperparameters are ML algorithm parameters used to control the learning process and are usually determined before the actual process starts. Hyperparameters should be set and tuned because they improve prediction quality and reproducibility. Tuning hyperparameters can lead to higher prediction quality because the default parameters of the learning algorithm may not be optimal for a given data or problem, and may lead to local optima. These parameters directly control the behavior of the training algorithm and therefore have a significant impact on the performance of the model.</description>
    </item>
    
    <item>
      <title>Excessive Hyperparameter</title>
      <link>https://hynn01.github.io/dslinter/deprecated/excessive-hyperparameter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/deprecated/excessive-hyperparameter/</guid>
      <description>Description Excessive hyperparameter precision is a potential risk for overtuning. Overtuning occurs if an overly high precision hyperparameter value allows the model to perform particularly well while values in close range of it do not. Further, the choice of such a hyperparameter might be uninterpretable to users, which leads to an untrustworthy result.
Type Generic
Existing Stage Model Training
Effect Error-prone
Example ### Scikit-Learn from sklearn.naive_bayes import MultinomialN # Violated Code mnb = MultinomialNB(alpha = 0.</description>
    </item>
    
    <item>
      <title>Counterintuitive Hyperparameter</title>
      <link>https://hynn01.github.io/dslinter/deprecated/counterintuitive-hyperparameter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/deprecated/counterintuitive-hyperparameter/</guid>
      <description>Description Counterintuitive hyperparameters will also cause bugs. There are four posts on StackOverflow discussing where the bug in the program is, and it turns out that a large learning rate causes bugs. This implies that the developer should check whether the hyperparameters stay in the normal range when developing ML applications.
Type Generic
Existing Stage Model Training
Effect Error-prone
Example # TensorFlow import tensorflow as tf # Violated Code optimizer = tf.</description>
    </item>
    
    <item>
      <title>Memory not Freed</title>
      <link>https://hynn01.github.io/dslinter/code-smells/memory-not-freed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/memory-not-freed/</guid>
      <description>Description ML application training is memory-consuming, and thus, it is essential to free memory in time. Some APIs are provided to alleviate the run-out-of-memory issue in deep learning libraries. TensorFlow&amp;rsquo;s documentation notes that if the model is created in a loop, it is suggested to use clear\_session() in the loop. Meanwhile, the GitHub repository &amp;ldquo;Pytorch best practice&amp;rdquo; recommends using .detach() to detach the tensor whenever possible. We suggest developers check whether they use these APIs to free the memory whenever possible in their code.</description>
    </item>
    
    <item>
      <title>Deterministic Algorithm Not Used</title>
      <link>https://hynn01.github.io/dslinter/code-smells/deterministic-algorithm-not-used/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/deterministic-algorithm-not-used/</guid>
      <description>Description Some libraries provide APIs for developers to use the deterministic algorithm. Using deterministic algorithms is another effort that can be made to improve reproducibility. In PyTorch, it is suggested to set torch.use_deterministic_algorithms(True) when debugging. However, the application will perform slower if this option is set, so it is suggested not to use it in the deploy stage. Developers should be aware of this setting during the development process.
Type Generic</description>
    </item>
    
    <item>
      <title>Missing the Mask of Invalid Value</title>
      <link>https://hynn01.github.io/dslinter/code-smells/missing-the-mask-of-invalid-value/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/missing-the-mask-of-invalid-value/</guid>
      <description>Description Several posts on Stack Overflow talk about the bugs that are not easy to discover caused by the log parameter approaching zero. In this kind of program, the log function variable turns to zero and raises an error during the training process. However, the error&amp;rsquo;s stack trace did not directly point to the line of code that the bug exist. This kind of problem is not easy to debug and might take a long training time to find.</description>
    </item>
    
    <item>
      <title>Nan Equality Misused</title>
      <link>https://hynn01.github.io/dslinter/code-smells/nan-equality-misused/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/nan-equality-misused/</guid>
      <description>Description While None == None evaluates to True, numpy.nan == numpy.nan evaluates to False. As Pandas treats None like numpy.nan for simplicity and performance reasons, a comparison of DataFrame elements with numpy.nan always return False . Therefore, developers need to be careful when using the NaN comparision in Numpy and Pandas. Otherwise, it may lead to unintentional behavior in the code.
Type API Specific
Existing Stage Data Preparation
Effect Error-prone</description>
    </item>
    
    <item>
      <title>Chain Indexing Misused</title>
      <link>https://hynn01.github.io/dslinter/code-smells/chain-indexing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/chain-indexing/</guid>
      <description>Avoid using chain indexing in Pandas.</description>
    </item>
    
    <item>
      <title>Dataframe Coversion API Misused</title>
      <link>https://hynn01.github.io/dslinter/code-smells/dataframe-coversion-api-misused/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/dataframe-coversion-api-misused/</guid>
      <description>Description When converting DataFrame to NumPy array, it is better to use df.to\_numpy() than df.values(). As noted in a post, df.values() has inconsistency problem. With .values it is unclear whether the returned value would be the actual array, some transformation of it, or one of the Pandas custom arrays. However, .values is not deprecated yet. Although the library developers note it as a warning in the documentation, it does not log a warning or error when compiling if we use .</description>
    </item>
    
    <item>
      <title>Regular Expression Solution Not Optimal</title>
      <link>https://hynn01.github.io/dslinter/deprecated/regular-expression-solution-not-optimal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/deprecated/regular-expression-solution-not-optimal/</guid>
      <description>Description A post provides a better coding solution for the regular expression, which might be able to apply to the Nature Language Processing (NLP) preprocessing process. NLP tasks are data-intensive and often take a long time to clean the data, so it would be good to save some time when preprocessing the data. The post noted that regex.sub() and str.translate() work faster than str.replace() in practice in Pandas library. The more efficient solution is worth considering.</description>
    </item>
    
    <item>
      <title>Tensor Shape Unset</title>
      <link>https://hynn01.github.io/dslinter/deprecated/tensor-shape-unset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/deprecated/tensor-shape-unset/</guid>
      <description>Description Several bugs can be caused by tensor unaligned. Usually, the unaligned tensor problems cannot be identified at the code level, but we can explicitly control one. It is recommended to explicitly set the shape of the input for tf.Variable() to make the tensor aligned. In this way, we can alleviate the unaligned tensor problem.
Type API Specific
Existing Stage Data Preparation
Effect Error-prone
Example ### TensorFlow import Tensorflow as tf # Violated Code normal_dist = tf.</description>
    </item>
    
    <item>
      <title>Backend Engine Mischoosed</title>
      <link>https://hynn01.github.io/dslinter/deprecated/backend-engine-mischoosed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/deprecated/backend-engine-mischoosed/</guid>
      <description>Description When using pd.eval() in Pandas library, the numexpr is optimized for performance and python options offer no performance benefit over numexpr. Generally, it is not recommended to set the parameter to python. The developers should be careful when explicitly set this parameter.
Type API Specific
Existing Stage Data Preparation
Effect Efficiency
Example ### Pandas import pandas as pd # Violated Code pd.eval(&amp;#39;df.A.str.contains(&amp;#34;ab&amp;#34;)&amp;#39;, engine=&amp;#39;python&amp;#39;) # Recommended Fix pd.eval(&amp;#39;df.A.str.contains(&amp;#34;ab&amp;#34;)&amp;#39;) Source: Paper Grey Literature GitHub Commit Stack Overflow  https://stackoverflow.</description>
    </item>
    
    <item>
      <title>DataLoader Unused</title>
      <link>https://hynn01.github.io/dslinter/code-smells/dataloader-unused/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/dataloader-unused/</guid>
      <description>Description Some new developers are not aware of using the existing function and writing the function by themselves. However, it is recommended to use the APIs because the APIs provided by the library often consider more things. For instance, in a post, the developer does not use the DataLoader and feeds the data directly to the network. The answerer noted that using DataLoader has several benefits: 1) It allows the developers to sample the data randomly.</description>
    </item>
    
    <item>
      <title>Matrix Multiplication API Misused</title>
      <link>https://hynn01.github.io/dslinter/code-smells/matrix-multiplication-api-misused/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/matrix-multiplication-api-misused/</guid>
      <description>Description np.matmul() in Numpy library is more readable than np.dot() in the semantic way. Whilenp.dot() provides heterogeneous behaviors depending on the shape of the data, np.matmul() behaves in a consistent way. When the multiply operation is performed on two-dimension matrixes, two APIs give a same result. However, np.matmul() is preferred than np.dot() for its clear semantic.
Type API Specific
Existing Stage Model Training
Effect Readability
Example ### NumPy import numpy as np a = [[1, 0], [0, 1]] b = [[4, 1], [2, 2]] - np.</description>
    </item>
    
    <item>
      <title>Initialization Order Misused</title>
      <link>https://hynn01.github.io/dslinter/deprecated/initialization-order-misused/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/deprecated/initialization-order-misused/</guid>
      <description>Description The AdamOptimizer class in the TensorFlow creates additional variables named &amp;ldquo;slots&amp;rdquo;. The variables must be initialized before training the model. Therefore, if the developer call initialize_all_variables() before calling AdamOptimizer and does not call the initializer afterward, the variables created by AdamOptimizer will not be initialized and might cause an error.
Type API Specific
Existing Stage Model Training
Effect Error-prone
Example ### TensorFlow import tensorflow as tf # Violated Code init = tf.</description>
    </item>
    
    <item>
      <title>Pytorch Call Method Misused</title>
      <link>https://hynn01.github.io/dslinter/code-smells/pytorch-call-method-misused/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/pytorch-call-method-misused/</guid>
      <description>Description In PyTorch, self.nn() is different than self.nn.forward(). self.nn() also deals with all the register hooks, which would not be considered when calling the plain forward. Thus, it is recommended to use self.nn() than self.nn.forward().
Type API Specific
Existing Stage Model Training
Effect Robustness
Example ### PyTorch # 1. Load and normalize CIFAR10 import torch import torchvision import torchvision.transforms as transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.</description>
    </item>
    
    <item>
      <title>Train Eval Mode Improper Toggling</title>
      <link>https://hynn01.github.io/dslinter/code-smells/train-eval-mode-improper-toggling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/train-eval-mode-improper-toggling/</guid>
      <description>Description In PyTorch, calling .eval() means we are going into the evaluation mode and the Dropout layer will be deactivated. If the training mode did not toggle back in time, the Dropout layer would not be used in some data training and thus affect the training result. Therefore, we suggest to &amp;ldquo;have the training mode set as close as possible to the inference step to avoid forgetting to set it&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Zero_grad Not Used Before Backward</title>
      <link>https://hynn01.github.io/dslinter/code-smells/zero_grad-not-used-before-backward/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/zero_grad-not-used-before-backward/</guid>
      <description>Description Developers should use optimizer.zero_grad(), loss_fn.backward(), optimizer.step() together and should be forget to use optimizer.zero_grad() before loss_fn.backward(). optimizer.zero_grad() clears the old gradients from last step. If this API is not used, the gradients will be accumulated from all loss.backward() calls and it will lead to the gradient explosion, which fails the training.
Type API Specific
Existing Stage Model Training
Effect Error-prone
Example # PyTorch # 1. Load and normalize CIFAR10 import torch import torchvision import torchvision.</description>
    </item>
    
    <item>
      <title>Loss API Misused</title>
      <link>https://hynn01.github.io/dslinter/code-smells/loss-api-misused/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/loss-api-misused/</guid>
      <description>Description Different loss APIs take different input formats, but the difference is not clarified in some documentations, so it is easy to misuse. For example, in PyTorch, the NLLLoss takes the output of LogSoftmax as the input. If the input given to NLLLoss has not been processed by LogSoftmax, it might lead to a wrong result.
Type API Specific
Existing Stage Model Training
Effect Error-prone
Example ### PyTorch import torch.nn as nn import torch + m = nn.</description>
    </item>
    
    <item>
      <title>Initialize Weight to a Constant</title>
      <link>https://hynn01.github.io/dslinter/deprecated/initialize-weight-to-zero/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/deprecated/initialize-weight-to-zero/</guid>
      <description>Description In neural network, if all the weights are initialized to a constant, i.e., all the neurons starts with the same weight, all the neurons will follow the same gradient during the backward propagation update. As a result, neurons will learn same features in each iterations. In this way, the nueral netwok will provide a poor result.
Type Generic
Existing Stage Model Training
Effect Error-prone
Example # https://github.com/aladdinpersson/Machine-Learning-Collection/blob/a2ee9271b5280be6994660c7982d0f44c67c3b63/ML/Projects/Exploring_MNIST/networks/lenet.py import torch import torch.</description>
    </item>
    
    <item>
      <title>Missing Regularization</title>
      <link>https://hynn01.github.io/dslinter/deprecated/missing-regularization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/deprecated/missing-regularization/</guid>
      <description>Description In deep learning, regularization can help control overfitting, speed up the training process by dealing with noise and outliers. Developers should check whether they are able to make use of regularization to improve performance.
Type Generic
Existing Stage Model Training
Effect Efficiency
Example ### TensorFlow # Violated Code layer = tf.keras.layers.Dense( 5, input_dim=5, kernel_initializer=&amp;#39;ones&amp;#39;, kernel_regularizer=tf.keras.regularizers.L1(0.01), activity_regularizer=tf.keras.regularizers.L2(0.01)) # Recommended Fix layer = tf.keras.layers.Dense( 5, input_dim=5, kernel_initializer=&amp;#39;ones&amp;#39;) ### PyTorch # Violated Code optimizer = torch.</description>
    </item>
    
    <item>
      <title>Threshold Dependent Validation</title>
      <link>https://hynn01.github.io/dslinter/code-smells/threshold-dependent-validation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/threshold-dependent-validation/</guid>
      <description>Description The performance of the machine learning model can be measured by different metrics, including threshold-dependent metrics(e.g., F-measure) or threshold-independent metrics(e.g., Area Under the receiver operating characteristic curve (AUC)). Choosing a specific threshold is tricky and can lead to a less-interpretable result. Therefore, threshold-independent is more robust and should be preferred over threshold-independent metrics.
Type Generic
Existing Stage Model Evaluation
Effect Robustness
Example ### Scikit-Learn from sklearn import metrics y_true = [0, 1, 2, 0, 1, 2] y_pred = [0, 2, 1, 0, 0, 1] metrics.</description>
    </item>
    
    <item>
      <title>Broadcasting Feature Not Used</title>
      <link>https://hynn01.github.io/dslinter/code-smells/broadcasting-feature-not-used/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/broadcasting-feature-not-used/</guid>
      <description>Description Context Problem Solution Type Existing Stage Effect Example ### TensorFlow example 1 import tensorflow as tf a = tf.constant([[1., 2.], [3., 4.]]) b = tf.constant([[1.], [2.]]) - c = a + tf.tile(b, [1, 2]) + c = a + b  ### TensorFlow example 2 import tensorflow as tf a = tf.random.uniform([5, 3, 5]) b = tf.random.uniform([5, 1, 6]) - tiled_b = tf.tile(b, [1, 3, 1]) - c = tf.</description>
    </item>
    
    <item>
      <title>Columns and DataType Not Explicitly Set</title>
      <link>https://hynn01.github.io/dslinter/code-smells/columns-and-datatype-not-explicitly-set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/columns-and-datatype-not-explicitly-set/</guid>
      <description>Description Context Problem Solution Type Existing Stage Effect Example import pandas as pd df = pd.read_csv(&amp;#39;data.csv&amp;#39;) + df = df[[&amp;#39;col1&amp;#39;, &amp;#39;col2&amp;#39;, &amp;#39;col3&amp;#39;]] Source: Paper Grey Literature GitHub Commit Stack Overflow Documentation </description>
    </item>
    
    <item>
      <title>Empty Column Misinitialization</title>
      <link>https://hynn01.github.io/dslinter/code-smells/empty-column-misinitialization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/empty-column-misinitialization/</guid>
      <description>Description Context Problem Solution Type Existing Stage Effect Example import pandas as pd + import numpy as np  df = pd.DataFrame([]) - df[&amp;#39;new_col_int&amp;#39;] = 0 - df[&amp;#39;new_col_str&amp;#39;] = &amp;#39;&amp;#39; + df[&amp;#39;new_col_float&amp;#39;] = np.nan + df[&amp;#39;new_col_int&amp;#39;] = pd.Series(dtype=&amp;#39;int&amp;#39;) + df[&amp;#39;new_col_str&amp;#39;] = pd.Series(dtype=&amp;#39;object&amp;#39;) Source: Paper Grey Literature GitHub Commit Stack Overflow Documentation </description>
    </item>
    
    <item>
      <title>Merge API Parameter Not Explicitly Set</title>
      <link>https://hynn01.github.io/dslinter/code-smells/merge-api-parameter-not-explicitly-set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/merge-api-parameter-not-explicitly-set/</guid>
      <description></description>
    </item>
    
    
    <item>
      <title>TensorArray Not Used</title>
      <link>https://hynn01.github.io/dslinter/code-smells/tensorarray-not-used/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hynn01.github.io/dslinter/code-smells/tensorarray-not-used/</guid>
      <description>Description Context Problem Solution Type Existing Stage Effect Example ### TensorFlow import tensorflow as tf @tf.function def fibonacci(n): a = tf.constant(1) b = tf.constant(1) - c = tf.constant([1, 1]) + c = tf.TensorArray(tf.int32, n) + c = c.write(0, a) + c = c.write(1, b)  for i in range(2, n): a, b = b, a + b - c = tf.concat([c, [b]], 0) +	c = c.write(i, b)  - return c +	return c.</description>
    </item>
    
  </channel>
</rss>
