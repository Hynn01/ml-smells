<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>No Scaling Before Scaling-sensitive Operation | DSLinter - Linter for Machine Learning - Specific Code Smells</title>
<meta name=keywords content="generic,data preparation,efficiency">
<meta name=description content="Description Principle Component Analysis (PCA) is used for finding the components that maximize the data&rsquo;s variation and reduce its dimensions, which is an essential data processing method. Scaling is pretty crucial to PCA because of the way the principal components are calculated. If one variable is on a larger scale than another, it will dominate the PCA procedure. Similarly, there are some other scaling-sensitive operations. Support Vector Machine (SVM), Stochastic Gradient Descent (SGD), Multi-layer Perceptron classifier, L1 and L2 regularization are all sensitive to feature scaling.">
<meta name=author content>
<link rel=canonical href=/code-smells/no-scaling-before-scaling-sensitive-operation/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.548091f41dc92b4a213f8dc4a49e22545a96b7d1b4ae4ad73c2ab3a70e4e8ea1.css integrity="sha256-VICR9B3JK0ohP43EpJ4iVFqWt9G0rkrXPCqzpw5OjqE=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.91.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
</noscript><meta property="og:title" content="No Scaling Before Scaling-sensitive Operation">
<meta property="og:description" content="Description Principle Component Analysis (PCA) is used for finding the components that maximize the data&rsquo;s variation and reduce its dimensions, which is an essential data processing method. Scaling is pretty crucial to PCA because of the way the principal components are calculated. If one variable is on a larger scale than another, it will dominate the PCA procedure. Similarly, there are some other scaling-sensitive operations. Support Vector Machine (SVM), Stochastic Gradient Descent (SGD), Multi-layer Perceptron classifier, L1 and L2 regularization are all sensitive to feature scaling.">
<meta property="og:type" content="article">
<meta property="og:url" content="/code-smells/no-scaling-before-scaling-sensitive-operation/"><meta property="article:section" content="Code Smells">
<meta property="og:site_name" content="DSLinter - Linter for Machine Learning Application - Specific Code Smells">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="No Scaling Before Scaling-sensitive Operation">
<meta name=twitter:description content="Description Principle Component Analysis (PCA) is used for finding the components that maximize the data&rsquo;s variation and reduce its dimensions, which is an essential data processing method. Scaling is pretty crucial to PCA because of the way the principal components are calculated. If one variable is on a larger scale than another, it will dominate the PCA procedure. Similarly, there are some other scaling-sensitive operations. Support Vector Machine (SVM), Stochastic Gradient Descent (SGD), Multi-layer Perceptron classifier, L1 and L2 regularization are all sensitive to feature scaling.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":3,"name":"No Scaling Before Scaling-sensitive Operation","item":"/code-smells/no-scaling-before-scaling-sensitive-operation/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"No Scaling Before Scaling-sensitive Operation","name":"No Scaling Before Scaling-sensitive Operation","description":"Description Principle Component Analysis (PCA) is used for finding the components that maximize the data\u0026rsquo;s variation and reduce its dimensions, which is an essential data processing method. Scaling is pretty crucial to PCA because of the way the principal components are calculated. If one variable is on a larger scale than another, it will dominate the PCA procedure. Similarly, there are some other scaling-sensitive operations. Support Vector Machine (SVM), Stochastic Gradient Descent (SGD), Multi-layer Perceptron classifier, L1 and L2 regularization are all sensitive to feature scaling.","keywords":["generic","data preparation","efficiency"],"articleBody":"Description Principle Component Analysis (PCA) is used for finding the components that maximize the dataâ€™s variation and reduce its dimensions, which is an essential data processing method. Scaling is pretty crucial to PCA because of the way the principal components are calculated. If one variable is on a larger scale than another, it will dominate the PCA procedure. Similarly, there are some other scaling-sensitive operations. Support Vector Machine (SVM), Stochastic Gradient Descent (SGD), Multi-layer Perceptron classifier, L1 and L2 regularization are all sensitive to feature scaling. To avoid bugs, whether feature scaling is added before these operations should be checked.\nType Generic\nExisting Stage Data Preparation\nEffect Efficiency\nExample ### Scikit-Learn PCA from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.decomposition import PCA from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score + from sklearn.preprocessing import StandardScaler  # Code source: Tyler Lanigan  # Sebastian Raschka  # License: BSD 3 clause # Make a train/test split using 30% test size RANDOM_STATE = 42 features, target = load_wine(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split( features, target, test_size=0.30, random_state=RANDOM_STATE ) # Fit to data and predict using pipeline - clf = make_pipeline(PCA(n_components=2), GaussianNB()) + clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB()) clf.fit(X_train, y_train) pred_test = clf.predict(X_test) ac = accuracy_score(y_test, pred_test) ### Scikit-Learn SVC from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.svm import SVC + from sklearn.pipeline import make_pipeline + from sklearn.preprocessing import StandardScaler  # Make a train/test split using 30% test size RANDOM_STATE = 42 features, target = load_wine(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split( features, target, test_size=0.30, random_state=RANDOM_STATE ) # Fit to data and predict using pipelined GNB and PCA - clf = SVC() + clf = make_pipeline(StandardScaler(), SVC()) clf.fit(X_train, y_train) pred_test = clf.predict(X_test) ac = accuracy_score(y_test, pred_test) Source: Paper Grey Literature  https://towardsdatascience.com/my-machine-learning-model-is-perfect-9a7928e0f604 https://ml.posthaven.com/machine-learning-done-wrong  GitHub Commit Stack Overflow  https://stackoverflow.com/questions/17455302/gridsearchcv-extremely-slow-on-small-dataset-in-scikit-learn/23813876#23813876  Documentation  https://scikit-learn.org/stable/modules/preprocessing.html https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-download-auto-examples-preprocessing-plot-scaling-importance-py  ","wordCount":"310","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"/code-smells/no-scaling-before-scaling-sensitive-operation/"},"publisher":{"@type":"Organization","name":"DSLinter - Linter for Machine Learning - Specific Code Smells","logo":{"@type":"ImageObject","url":"%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href accesskey=h title="DSLinter - Linter for Machine Learning - Specific Code Smells (Alt + H)">DSLinter - Linter for Machine Learning - Specific Code Smells</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=/code-smells/ title="Code Smells">
<span>Code Smells</span>
</a>
</li>
<li>
<a href=search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
<li>
<a href title=Survey>
<span>Survey</span>
</a>
</li>
<li>
<a href=/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href>Home</a></div>
<h1 class=post-title>
No Scaling Before Scaling-sensitive Operation
</h1>
<div class=post-meta>
</div>
</header>
<div class=post-content><h3 id=description>Description<a hidden class=anchor aria-hidden=true href=#description>#</a></h3>
<p>Principle Component Analysis (PCA) is used for finding the components that maximize the data&rsquo;s variation and reduce its dimensions, which is an essential data processing method. Scaling is pretty crucial to PCA because of the way the principal components are calculated. If one variable is on a larger scale than another, it will dominate the PCA procedure. Similarly, there are some other scaling-sensitive operations. Support Vector Machine (SVM), Stochastic Gradient Descent (SGD), Multi-layer Perceptron classifier, L1 and L2 regularization are all sensitive to feature scaling. To avoid bugs, whether feature scaling is added before these operations should be checked.</p>
<h3 id=type>Type<a hidden class=anchor aria-hidden=true href=#type>#</a></h3>
<p>Generic</p>
<h3 id=existing-stage>Existing Stage<a hidden class=anchor aria-hidden=true href=#existing-stage>#</a></h3>
<p>Data Preparation</p>
<h3 id=effect>Effect<a hidden class=anchor aria-hidden=true href=#effect>#</a></h3>
<p>Efficiency</p>
<h3 id=example>Example<a hidden class=anchor aria-hidden=true href=#example>#</a></h3>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff>### Scikit-Learn PCA
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import PCA
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
<span style=color:#a6e22e>+ from sklearn.preprocessing import StandardScaler
</span><span style=color:#a6e22e></span>
# Code source: Tyler Lanigan &lt;tylerlanigan@gmail.com&gt;
#              Sebastian Raschka &lt;mail@sebastianraschka.com&gt;
# License: BSD 3 clause

# Make a train/test split using 30% test size
RANDOM_STATE = 42
features, target = load_wine(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    features, target, test_size=0.30, random_state=RANDOM_STATE
)

# Fit to data and predict using pipeline
<span style=color:#f92672>- clf = make_pipeline(PCA(n_components=2), GaussianNB())
</span><span style=color:#f92672></span><span style=color:#a6e22e>+ clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB())
</span><span style=color:#a6e22e></span>clf.fit(X_train, y_train)
pred_test = clf.predict(X_test)
ac = accuracy_score(y_test, pred_test)


### Scikit-Learn SVC
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
<span style=color:#a6e22e>+ from sklearn.pipeline import make_pipeline
</span><span style=color:#a6e22e>+ from sklearn.preprocessing import StandardScaler
</span><span style=color:#a6e22e></span>
# Make a train/test split using 30% test size
RANDOM_STATE = 42
features, target = load_wine(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    features, target, test_size=0.30, random_state=RANDOM_STATE
)

# Fit to data and predict using pipelined GNB and PCA
<span style=color:#f92672>- clf = SVC()
</span><span style=color:#f92672></span><span style=color:#a6e22e>+ clf = make_pipeline(StandardScaler(), SVC())
</span><span style=color:#a6e22e></span>clf.fit(X_train, y_train)
pred_test = clf.predict(X_test)
ac = accuracy_score(y_test, pred_test)
</code></pre></div><h3 id=source>Source:<a hidden class=anchor aria-hidden=true href=#source>#</a></h3>
<h4 id=paper>Paper<a hidden class=anchor aria-hidden=true href=#paper>#</a></h4>
<h4 id=grey-literature>Grey Literature<a hidden class=anchor aria-hidden=true href=#grey-literature>#</a></h4>
<ul>
<li><a href=https://towardsdatascience.com/my-machine-learning-model-is-perfect-9a7928e0f604>https://towardsdatascience.com/my-machine-learning-model-is-perfect-9a7928e0f604</a></li>
<li><a href=https://ml.posthaven.com/machine-learning-done-wrong>https://ml.posthaven.com/machine-learning-done-wrong</a></li>
</ul>
<h4 id=github-commit>GitHub Commit<a hidden class=anchor aria-hidden=true href=#github-commit>#</a></h4>
<h4 id=stack-overflow>Stack Overflow<a hidden class=anchor aria-hidden=true href=#stack-overflow>#</a></h4>
<ul>
<li><a href=https://stackoverflow.com/questions/17455302/gridsearchcv-extremely-slow-on-small-dataset-in-scikit-learn/23813876#23813876>https://stackoverflow.com/questions/17455302/gridsearchcv-extremely-slow-on-small-dataset-in-scikit-learn/23813876#23813876</a></li>
</ul>
<h4 id=documentation>Documentation<a hidden class=anchor aria-hidden=true href=#documentation>#</a></h4>
<ul>
<li><a href=https://scikit-learn.org/stable/modules/preprocessing.html>https://scikit-learn.org/stable/modules/preprocessing.html</a></li>
<li><a href=https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-download-auto-examples-preprocessing-plot-scaling-importance-py>https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-download-auto-examples-preprocessing-plot-scaling-importance-py</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=/tags/generic/>generic</a></li>
<li><a href=/tags/data-preparation/>data preparation</a></li>
<li><a href=/tags/efficiency/>efficiency</a></li>
</ul>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href>DSLinter - Linter for Machine Learning - Specific Code Smells</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>