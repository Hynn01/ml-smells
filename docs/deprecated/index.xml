<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Deprecateds on DSLinter - Linter for Machine Learning - Specific Code Smells</title>
    <link>/deprecated/</link>
    <description>Recent content in Deprecateds on DSLinter - Linter for Machine Learning - Specific Code Smells</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="/deprecated/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Excessive Hyperparameter</title>
      <link>/deprecated/excessive-hyperparameter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/deprecated/excessive-hyperparameter/</guid>
      <description>Description Excessive hyperparameter precision is a potential risk for overtuning. Overtuning occurs if an overly high precision hyperparameter value allows the model to perform particularly well while values in close range of it do not. Further, the choice of such a hyperparameter might be uninterpretable to users, which leads to an untrustworthy result.
Type Generic
Existing Stage Model Training
Effect Error-prone
Example ### Scikit-Learn from sklearn.naive_bayes import MultinomialN # Violated Code mnb = MultinomialNB(alpha = 0.</description>
    </item>
    
    <item>
      <title>Counterintuitive Hyperparameter</title>
      <link>/deprecated/counterintuitive-hyperparameter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/deprecated/counterintuitive-hyperparameter/</guid>
      <description>Description Counterintuitive hyperparameters will also cause bugs. There are four posts on StackOverflow discussing where the bug in the program is, and it turns out that a large learning rate causes bugs. This implies that the developer should check whether the hyperparameters stay in the normal range when developing ML applications.
Type Generic
Existing Stage Model Training
Effect Error-prone
Example # TensorFlow import tensorflow as tf # Violated Code optimizer = tf.</description>
    </item>
    
    <item>
      <title>Regular Expression Solution Not Optimal</title>
      <link>/deprecated/regular-expression-solution-not-optimal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/deprecated/regular-expression-solution-not-optimal/</guid>
      <description>Description A post provides a better coding solution for the regular expression, which might be able to apply to the Nature Language Processing (NLP) preprocessing process. NLP tasks are data-intensive and often take a long time to clean the data, so it would be good to save some time when preprocessing the data. The post noted that regex.sub() and str.translate() work faster than str.replace() in practice in Pandas library. The more efficient solution is worth considering.</description>
    </item>
    
    <item>
      <title>Tensor Shape Unset</title>
      <link>/deprecated/tensor-shape-unset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/deprecated/tensor-shape-unset/</guid>
      <description>Description Several bugs can be caused by tensor unaligned. Usually, the unaligned tensor problems cannot be identified at the code level, but we can explicitly control one. It is recommended to explicitly set the shape of the input for tf.Variable() to make the tensor aligned. In this way, we can alleviate the unaligned tensor problem.
Type API Specific
Existing Stage Data Preparation
Effect Error-prone
Example ### TensorFlow import Tensorflow as tf # Violated Code normal_dist = tf.</description>
    </item>
    
    <item>
      <title>Backend Engine Mischoosed</title>
      <link>/deprecated/backend-engine-mischoosed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/deprecated/backend-engine-mischoosed/</guid>
      <description>Description When using pd.eval() in Pandas library, the numexpr is optimized for performance and python options offer no performance benefit over numexpr. Generally, it is not recommended to set the parameter to python. The developers should be careful when explicitly set this parameter.
Type API Specific
Existing Stage Data Preparation
Effect Efficiency
Example ### Pandas import pandas as pd # Violated Code pd.eval(&amp;#39;df.A.str.contains(&amp;#34;ab&amp;#34;)&amp;#39;, engine=&amp;#39;python&amp;#39;) # Recommended Fix pd.eval(&amp;#39;df.A.str.contains(&amp;#34;ab&amp;#34;)&amp;#39;) Source: Paper Grey Literature GitHub Commit Stack Overflow  https://stackoverflow.</description>
    </item>
    
    <item>
      <title>Initialization Order Misused</title>
      <link>/deprecated/initialization-order-misused/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/deprecated/initialization-order-misused/</guid>
      <description>Description The AdamOptimizer class in the TensorFlow creates additional variables named &amp;ldquo;slots&amp;rdquo;. The variables must be initialized before training the model. Therefore, if the developer call initialize_all_variables() before calling AdamOptimizer and does not call the initializer afterward, the variables created by AdamOptimizer will not be initialized and might cause an error.
Type API Specific
Existing Stage Model Training
Effect Error-prone
Example ### TensorFlow import tensorflow as tf # Violated Code init = tf.</description>
    </item>
    
    <item>
      <title>Initialize Weight to a Constant</title>
      <link>/deprecated/initialize-weight-to-zero/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/deprecated/initialize-weight-to-zero/</guid>
      <description>Description In neural network, if all the weights are initialized to a constant, i.e., all the neurons starts with the same weight, all the neurons will follow the same gradient during the backward propagation update. As a result, neurons will learn same features in each iterations. In this way, the nueral netwok will provide a poor result.
Type Generic
Existing Stage Model Training
Effect Error-prone
Example # https://github.com/aladdinpersson/Machine-Learning-Collection/blob/a2ee9271b5280be6994660c7982d0f44c67c3b63/ML/Projects/Exploring_MNIST/networks/lenet.py import torch import torch.</description>
    </item>
    
    <item>
      <title>Missing Regularization</title>
      <link>/deprecated/missing-regularization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/deprecated/missing-regularization/</guid>
      <description>Description In deep learning, regularization can help control overfitting, speed up the training process by dealing with noise and outliers. Developers should check whether they are able to make use of regularization to improve performance.
Type Generic
Existing Stage Model Training
Effect Efficiency
Example ### TensorFlow # Violated Code layer = tf.keras.layers.Dense( 5, input_dim=5, kernel_initializer=&amp;#39;ones&amp;#39;, kernel_regularizer=tf.keras.regularizers.L1(0.01), activity_regularizer=tf.keras.regularizers.L2(0.01)) # Recommended Fix layer = tf.keras.layers.Dense( 5, input_dim=5, kernel_initializer=&amp;#39;ones&amp;#39;) ### PyTorch # Violated Code optimizer = torch.</description>
    </item>
    
  </channel>
</rss>
