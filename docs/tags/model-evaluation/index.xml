<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>model evaluation on DSLinter - Linter for Machine Learning - Specific Code Smells</title>
    <link>/tags/model-evaluation/</link>
    <description>Recent content in model evaluation on DSLinter - Linter for Machine Learning - Specific Code Smells</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="/tags/model-evaluation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Threshold Dependent Validation</title>
      <link>/code-smells/threshold-dependent-validation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/code-smells/threshold-dependent-validation/</guid>
      <description>Description The performance of the machine learning model can be measured by different metrics, including threshold-dependent metrics(e.g., F-measure) or threshold-independent metrics(e.g., Area Under the receiver operating characteristic curve (AUC)). Choosing a specific threshold is tricky and can lead to a less-interpretable result. Therefore, threshold-independent is more robust and should be preferred over threshold-independent metrics.
Type Generic
Existing Stage Model Evaluation
Effect Robustness
Example ### Scikit-Learn from sklearn import metrics y_true = [0, 1, 2, 0, 1, 2] y_pred = [0, 2, 1, 0, 0, 1] metrics.</description>
    </item>
    
  </channel>
</rss>
