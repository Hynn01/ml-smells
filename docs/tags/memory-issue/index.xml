<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>memory issue on DSLinter - Linter for Machine Learning - Specific Code Smells</title>
    <link>/tags/memory-issue/</link>
    <description>Recent content in memory issue on DSLinter - Linter for Machine Learning - Specific Code Smells</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="/tags/memory-issue/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Memory not Freed</title>
      <link>/code-smells/memory-not-freed/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/code-smells/memory-not-freed/</guid>
      <description>Description ML application training is memory-consuming, and thus, it is essential to free memory in time. Some APIs are provided to alleviate the run-out-of-memory issue in deep learning libraries. TensorFlow&amp;rsquo;s documentation notes that if the model is created in a loop, it is suggested to use clear\_session() in the loop. Meanwhile, the GitHub repository &amp;ldquo;Pytorch best practice&amp;rdquo; recommends using .detach() to detach the tensor whenever possible. We suggest developers check whether they use these APIs to free the memory whenever possible in their code.</description>
    </item>
    
  </channel>
</rss>
